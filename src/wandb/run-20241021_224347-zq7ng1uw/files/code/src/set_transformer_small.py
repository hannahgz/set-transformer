"""set_transformer_small.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bvaQwDegrF9u6At559hZo2VhyR0SKcVa

# Fixed Dataset Construction
"""

import os
import torch
from torch import optim
from torch.utils.data import DataLoader
import wandb
from model import GPT, GPTConfig
from tokenizer import Tokenizer
from set_dataset import SetDataset, BalancedSetDataset
from data_utils import generate_cont_combinations, generate_combinations, separate_sets_non_sets


# lr = 1e-3
# epochs = 2
# batch_size = 32
# n_layer = 4
# n_head = 4
# n_embd = 128
# patience = 3
# # eval_freq = 26
# eval_freq = 0

device = "cuda" if torch.cuda.is_available() else "cpu"


def wandb_log(
    avg_train_loss, avg_val_loss, total_training_samples_seen=None, epoch=None
):
    if total_training_samples_seen is not None:
        print(
            f"Epoch {epoch+1}/{GPTConfig.epochs}, Training Samples Seen {total_training_samples_seen}, \
            Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}"
        )

        wandb.log(
            {
                "training_samples_seen": total_training_samples_seen,
                "train_loss": avg_train_loss,
                "val_loss": avg_val_loss,
            }
        )
    else:
        print(
            f"Epoch {epoch+1}/{GPTConfig.epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}"
        )

        wandb.log(
            {
                "epoch": epoch + 1,
                "train_loss": avg_train_loss,
                "val_loss": avg_val_loss,
            }
        )


# Update accuracy calculation
# TODO: check that this is implemented correctly
# TODO: add alternate measure for computing accuracy (set prediction, but wrong order of cards)
@torch.no_grad()
def calculate_accuracy(model, dataloader, padding_token):
    model.eval()
    correct = 0
    total = 0

    for sequences in dataloader:
        inputs = sequences[:, : GPTConfig().input_size].to(device)
        targets = sequences[:, GPTConfig().input_size :].to(device)

        outputs = model.generate(inputs, max_new_tokens=GPTConfig().target_size)
        predictions = outputs[:, GPTConfig().input_size :]

        print("targets: ", targets)
        print("predictions: ", predictions)

        # breakpoint()

        mask = targets != padding_token  # Create a mask to ignore padding
        correct += ((predictions == targets) | ~mask).all(dim=1).sum().item()
        total += mask.any(dim=1).sum().item()

    return correct / total


def evaluate_val_loss(
    model,
    val_loader,
    optimizer,
    counter,
    best_val_loss,
    val_losses,
    total_training_samples_seen=None,
    epoch=None,
):
    model.eval()
    total_val_loss = 0
    avg_val_loss = 0
    with torch.no_grad():
        for inputs in val_loader:
            inputs = inputs.to(device)
            _, loss = model(inputs, True)
            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    # if avg_val_loss < best_val_loss or always_save_checkpoint:
    if avg_val_loss < best_val_loss:
        counter = 0
        best_val_loss = avg_val_loss
        if total_training_samples_seen is not None:
            checkpoint = {
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "training_samples_seen": total_training_samples_seen,
                "best_val_loss": best_val_loss,
                "config": config,
            }
        else:
            checkpoint = {
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "epoch_num": epoch,
                "best_val_loss": best_val_loss,
                "config": config,
            }
        print(f"saving checkpoint to {out_dir}")
        torch.save(checkpoint, os.path.join(out_dir, "zxcv.pt"))
    else:
        counter += 1

    return avg_val_loss, best_val_loss, counter


def run():

    wandb.init(
       project="set-prediction-small",
       config={
           "learning_rate": GPTConfig().lr,
           "epochs": GPTConfig().epochs,
           "batch_size": GPTConfig().batch_size,
           "n_layer": GPTConfig().n_layer,
           "n_head": GPTConfig().n_head,
           "n_embd": GPTConfig().n_embd,
           "patience": GPTConfig().patience,
           "eval_freq": GPTConfig().eval_freq,
       },
    )

    wandb.finish()

    # # # Use the generator instead of storing all combinations in memory
    # optimized_combinations = generate_combinations(
    #     GPTConfig().target_size, GPTConfig().pad_symbol, GPTConfig().n_cards
    # )
    
    # small_combinations = list(optimized_combinations)
    


    # # Create tokenizer and tokenize all sequences
    # tokenizer = Tokenizer()
    # tokenized_combinations = [tokenizer.encode(seq) for seq in small_combinations]
    # end_of_seq_token = tokenized_combinations[0][-1]
    # padding_token = 0
    # no_set_token = 0

    # for i in range(len(small_combinations)):
    #     if GPTConfig().pad_symbol in small_combinations[i]:
    #         padding_token_pos = small_combinations[i].index(GPTConfig().pad_symbol)
    #         padding_token = tokenized_combinations[i][padding_token_pos]

    #         no_set_token_pos = small_combinations[i].index("*")
    #         no_set_token = tokenized_combinations[i][no_set_token_pos]
    #         break

    # print("padding token: ", padding_token)
    # print("end of seq token: ", end_of_seq_token)
    # print("no set token: ", no_set_token)

    # # Separate out sets from non sets in the tokenized representation
    # set_sequences, non_set_sequences = separate_sets_non_sets(tokenized_combinations, no_set_token, -4)

    # # Create dataset and dataloaders
    # # dataset = SetDataset(tokenized_combinations)
    # # train_size = int(0.95 * len(dataset))
    # dataset = BalancedSetDataset(set_sequences, non_set_sequences)
    # train_size = int(0.9 * len(dataset))  # makes val size 1296

    # # make validation set a lot smaller TODO, revisit how large val set this leaves us with
    # val_size = len(dataset) - train_size
    # train_dataset, val_dataset = torch.utils.data.random_split(
    #     dataset, [train_size, val_size]
    # )

    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # # Update model initialization
    # config = GPTConfig(
    #     n_layer=n_layer,
    #     n_head=n_head,
    #     n_embd=n_embd,
    #     vocab_size=len(tokenizer.token_to_id),
    # )

    # config.end_of_seq_token = end_of_seq_token
    # config.padding_token = padding_token
    # device = "cuda" if torch.cuda.is_available() else "cpu"
    # model = GPT(config).to(device)
    # optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)

    # # Training loop (remains mostly the same)
    # train_losses = []
    # val_losses = []

    # best_val_loss = 1e9
    # out_dir = ""

    # counter = 0
    # total_training_samples_seen = 0
    # break_flag = False
    # for epoch in range(epochs):
    #     if break_flag or (not eval_freq and counter >= patience):
    #         break
    #     model.train()
    #     total_train_loss = 0
    #     for index, inputs in enumerate(
    #         train_loader
    #     ):  # train_loader, 364 batches (11664/32)
    #         model.train()
    #         inputs = inputs.to(device)
    #         optimizer.zero_grad()
    #         _, loss = model(inputs, True)
    #         loss.backward()
    #         optimizer.step()
    #         total_train_loss += loss.item()
    #         total_training_samples_seen += len(inputs)
    #         if not eval_freq or index % eval_freq != 0:
    #             continue

    #         if counter >= patience:
    #             break_flag = True
    #             break

    #         epoch_seen_samples = (index * batch_size) + len(inputs)
    #         avg_train_loss = total_train_loss / epoch_seen_samples
    #         train_losses.append(avg_train_loss)

    #         avg_val_loss, best_val_loss, counter = evaluate_val_loss(
    #             model,
    #             val_loader,
    #             optimizer,
    #             counter,
    #             best_val_loss,
    #             val_losses,
    #             total_training_samples_seen,
    #         )

    #     if eval_freq:
    #         continue

    #     avg_train_loss = total_train_loss / len(train_loader)
    #     train_losses.append(avg_train_loss)

    #     avg_val_loss, best_val_loss, counter = evaluate_val_loss(
    #         model,
    #         val_loader,
    #         optimizer,
    #         counter,
    #         best_val_loss,
    #         val_losses,
    #         epoch=epoch,
    #     )
        
    # # Comment out if not loading already trained model
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # # checkpoint_path = "zxcv.pt"
    # # checkpoint_path = "ckpt_small_patience (1).pt"
    # # config = GPTConfig(vocab_size=len(tokenizer.token_to_id))
    # # model = GPT(config).to(device)
    # # checkpoint = torch.load(checkpoint_path)
    # # Restore the model state dict
    # # model.load_state_dict(checkpoint["model"])

    # train_accuracy = calculate_accuracy(model, train_loader)
    # val_accuracy = calculate_accuracy(model, val_loader)

    # print(f"Train Accuracy: {train_accuracy:.4f}")
    # print(f"Validation Accuracy: {val_accuracy:.4f}")

    # #wandb.log({"train_accuracy": train_accuracy, "val_accuracy": val_accuracy})

    # #wandb.finish()



if __name__ == "__main__":
    run()
    # optimized_combinations = run()
